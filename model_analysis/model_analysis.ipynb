{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../data/rating.csv.zip')\n",
    "animes = pd.read_csv('../data/anime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaning ratings to only allow values greater than -1\n",
    "#Cleaning animes to only allow TV Shows\n",
    "ratings = ratings[ratings.rating > -1]\n",
    "animes = animes[animes.type == 'TV']\n",
    "ratings = ratings[ratings.anime_id.isin(animes.anime_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print animes.shape\n",
    "print ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting the amount of differents users\n",
    "nb_users = ratings.groupby(\"user_id\").user_id.nunique().shape[0]\n",
    "#Getting the amount of anime shows\n",
    "nb_shows = animes.shape[0]\n",
    "print nb_users\n",
    "print nb_shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Rewritting ratings dataframe to ajust user_id in sequences\n",
    "#We have 69600 users in ratings but we have 73516 different users id's\n",
    "#We need to create a test set, but we'll have data loss if we don't have the user_id in sequences\n",
    "actual_index = ratings.iloc[0,].user_id\n",
    "new_index = 0\n",
    "new_list_user_id = []\n",
    "for i in range(ratings.shape[0]):\n",
    "    if i % 120000 == 0:\n",
    "        print\"{0} %\".format((i / float(ratings.shape[0])) * 100.)\n",
    "    if ratings.iloc[i,].user_id != actual_index:\n",
    "        new_index += 1\n",
    "        actual_index = ratings.iloc[i,].user_id    \n",
    "    new_list_user_id.append(new_index)\n",
    "print actual_index\n",
    "print new_index        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Doing the same structure for anime data\n",
    "mapped_anime_ids = {}\n",
    "for i in range(animes.shape[0]):\n",
    "    mapped_anime_ids[animes.iloc[i,].anime_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ratings.shape[0]):\n",
    "    if i % 120000 == 0:\n",
    "        print\"{0} %\".format((i / float(ratings.shape[0])) * 100.)\n",
    "    ratings.iloc[i,].anime_id = mapped_anime_ids[ratings.iloc[i,].anime_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings['user_id'] = new_list_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exporting new data set.\n",
    "ratings.to_csv('ratings_corrected.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Working with corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_corrected = pd.read_csv('./ratings_corrected.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Training and Test Set with Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set, test_set = train_test_split(ratings_corrected, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_set(data, n_rows, n_cols):\n",
    "        process_set = []\n",
    "        for i  in range(n_rows):\n",
    "            rated = data[data.user_id == i]\n",
    "            animes_rated = rated.anime_id\n",
    "            ratings_obtained = rated.rating\n",
    "            ratings = np.zeros(n_cols)\n",
    "            ratings[animes_rated] = ratings_obtained\n",
    "\n",
    "            process_set.append(list(ratings))\n",
    "        return process_set            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = convert_set(training_set, nb_users, nb_shows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = convert_set(test_set, nb_users, nb_shows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Stacker AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from types import *\n",
    "import sys\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, input_output_size, encoder_input=20, decoder_input=20):\n",
    "        super(SAE, self).__init__()\n",
    "        self.encoder = nn.Linear(input_output_size, encoder_input)\n",
    "        self.hidden_layers = []\n",
    "        self.decoder = nn.Linear(decoder_input, input_output_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.last_out = encoder_input\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #First step\n",
    "        x = self.activation(self.encoder(x))\n",
    "        #We only handle Linear and Dropout layers\n",
    "        for layer in self.hidden_layers:\n",
    "            if \"Linear\" in str(type(layer)):\n",
    "                #It's a linear layer\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                #It's a dropout layer\n",
    "                x = layer(x)\n",
    "        #Final Step\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def add_hiden_layer(self, out_features):\n",
    "        new_layer = nn.Linear(self.last_out, out_features)\n",
    "        self.last_out = out_features\n",
    "        self.hidden_layers.append(new_layer)\n",
    "        \n",
    "    def add_dropout(self, p=0.5):\n",
    "        new_dropout = nn.Dropout(p)\n",
    "        self.hidden_layers.append(new_dropout)\n",
    "        \n",
    "    def print_progress(self, message):\n",
    "        sys.stdout.write(\"\\r\" + message)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def compile(optimizer='rmsprop', criterion='mse'):\n",
    "        if type(criterion) is StringType:\n",
    "            if criterion == 'mse':\n",
    "                self.criterion = nn.MSELoss()\n",
    "            else:\n",
    "                self.criterion = nn.L1Loss()\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "            if type(optimizer) is StringType:\n",
    "                pass\n",
    "            else:\n",
    "                self.optimizer = optimizer\n",
    "            \n",
    "    def train(self, X, nb_epoch):\n",
    "        for epoch in range(nb_epoch):\n",
    "            train_loss = 0\n",
    "            s = 0.\n",
    "            rows, columns = X.size()\n",
    "            for index in range(int(row)):\n",
    "                input = Variable(X[index]).unsqueeze(0)\n",
    "                target = input.clone()\n",
    "                if torch.sum(target.data > 0) > 0:\n",
    "                    output = self.forward(input)\n",
    "                    target.require_grad = False\n",
    "                    output[target == 0] = 0\n",
    "                    loss = self.criterion(output, target)\n",
    "                    mean_corrector = columns/float(torch.sum(target.data > 0) + 1e-10)\n",
    "                    loss.backward()\n",
    "                    train_loss += np.sqrt(loss.data[0] * mean_corrector)\n",
    "                    s += 1.\n",
    "                    optimizer.step()\n",
    "                self.print_progress(\"epoch: {0}, training: {1}/{2}\".format(epoch + 1, index + 1, rows))\n",
    "            self.print_progress('epoch: {0}, training loss{1}'.format(epoch + 1, train_loss / s))\n",
    "            sys.stdout.write(\"\\n\" + message)\n",
    "            sys.stdout.flush()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(nb_epoch):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_shows/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
